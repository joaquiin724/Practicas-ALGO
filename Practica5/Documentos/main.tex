\documentclass[11pt,openany]{book}
\input{assets/portada.tex}
\usepackage{assets/formulas}
\usepackage[mathscr]{euscript}
\usepackage{bm}
\usepackage{float}
\hbadness=10000 % Suppress Underfull \hbox warnings

%========================================|Indice|===============================================%

\begin{document}
\portada{Algorítmica}{2023-2024}{Miguel Ángel De la Vega Rodríguez}{https://github.com/Miguevrgo/}{github.png}
\tableofcontents % Índice
\newpage %Salto de pagina tras el Indice


%======================================|Documento|==============================================%
\chapter{Autores}
\begin{itemize}
      \item \textbf{Miguel Ángel De la Vega Rodríguez:} 20\%
            \begin{itemize}
                  \item Estructura del documento
                  \item Ejemplos de Uso
            \end{itemize}
      \item \textbf{Joaquín Avilés De la Fuente:} 20\%
            \begin{itemize}
                  \item RELLENAR
            \end{itemize}
      \item \textbf{Alberto De la Vera Sánchez: } 20\%
            \begin{itemize}
                  \item RELLENAR
            \end{itemize}
      \item \textbf{Manuel Gomez Rubio} 20\%
            \begin{itemize}
                \item RELLENAR
            \end{itemize}
      \item \textbf{Pablo Linari Pérez:} 20\%
            \begin{itemize}
                  \item RELLENAR
            \end{itemize}
\end{itemize}

\chapter{Equipo de trabajo}

\begin{itemize}
      \item \textbf{Miguel Ángel De la Vega Rodríguez:} (Ordenador donde se ha realizado el computo)
            \begin{itemize}
                  \item AMD Ryzen 7 2700X 8-Core
                  \item 16 GB RAM DDR4 3200 MHz
                  \item NVIDIA GeForce GTX 1660 Ti
                  \item 1 TB SSD NvMe
                  \item Debian 12 Bookworm
                  \item Compilador GCC 12.2.0
            \end{itemize}
\end{itemize}

\chapter{Introducción}
El algoritmo de Viterbi es un algoritmo de programación dinámica utilizado para encontrar la secuencia de estados más probable en un \textbf{modelo oculto de Markov 
(HMM, Hidden Markov Model)}. El algoritmo de Viterbi es ampliamente utilizado en diversos campos, como el lenguaje, la ingeniería de comunicaciones, la robótica, la 
biología, la medicina, la meteorología, etc. \\ \\
En este documento, presentamos una descripción detallada del algoritmo de Viterbi, su implementación y ejemplos de uso 
en diferentes campos, haciendo sobre todo énfasis en el uso de la programaciónn dinámica. 

\chapter{Descripción del Algoritmo}
Como bien hemos comentado en la introducción el algoritmo de Viterbi es un algoritmo de programación dinámica utilizado para encontrar la secuencia de estados más probable en un 
modelo oculto de Markov (HMM, Hidden Markov Model), por lo que en primer lugar pasaremos a explicar dicho modelo así como los elementos que lo componen y su sintaxis matemática, 
para poder comprender mejor el algoritmo de Viterbi y sus ejemplos.

\section{Modelo Oculto de Markov (HMM)}
Un modelo oculto de Markov (HMM) es un modelo estadístico que describe la secuencia de estados a través de la cual pasa un proceso estocástico. En un HMM, el proceso estocástico
es un proceso de Markov, lo que significa que la probabilidad de que el sistema pase a un estado futuro depende únicamente del estado actual y no de los estados anteriores, de forma que 
el calculo del estado más probables se hace mediante la probabilidad de transición entre estados y de la probabilidad de emisión de observaciones. \\ \\
Un HHM se representa mediante una tupla $(Q, V, \pi, A, B,)$ y explicaremos a continuación sus componentes:
\begin{itemize}
      \item Un conjunto de estados $Q = \{q_1, q_2, \ldots, q_N\}$.
      \item Un conjunto de estados observables $V = \{v_1, v_2, \ldots, v_L\}$.
      \item Un conjunto de probabilidades de transición entre estados $A = \{a_{ij}\}$, donde $a_{ij} = P(q_t = j | q_{t-1} = i)$, es decir, la probabilidad de 
      pasar de un estado $i$ en el instante de tiempo $t-1$ a un estado $j$ en el instante de tiempo $t$.
      \item Un conjunto de probabilidades de las observaciones $B = \{b_j(v_k)\}$, donde $b_j(v_k) = P(o_t = v_k | q_t = j)$, es decir, la probailidad de observar
      $v_k$ en el estado $j$ en el instante de tiempo $t$.
      \item Un conjunto de probabilidades de estados iniciales $\pi = \{\pi_i\}$, donde $\pi_i = P(q_1 = i)$, es decir, la probabilidad de que en el instante $1$ (al inicio) se tenga
      el estado $i$.
\end{itemize}
 
En dicho modelo, notaremos a la secuencia de observaciones como el conjunto $O=\{o_1, o_2, \ldots, o_L\}$ y al conjunto
de estados ocultos con mayor probabilidad como $S = \{s_1, s_2, \ldots, s_L\}$, donde $L$ es el número de observaciones, por lo que como se podía deducir se tiene
el mismo número de observaciones que estados ocultos en la secuencia con mayor probabilidad, pues estos estados serán consecuencia (en parte) de las observaciones dadas. \\ \\

\section{Algoritmo de Viterbi}
En esta sección procederemos a explicar el \textbf{Algoritmo de Viterbi}, donde daremos una explicación detallada de su funcionamiento y del uso de programación dinámica en el mismo. \\ \\
El algoritmo de Viterbi se basa en la idea de que la probabilidad de la secuencia de estados más probable hasta el instante de tiempo $t$ se puede calcular a partir de la probabilidad
de la secuencia de estados más probable hasta el instante de tiempo $t-1$. \\ \\
El algoritmo de Viterbi se puede resumir en los siguientes pasos:
\begin{itemize}
      \item Inicialización: Calcular la probabilidad de la secuencia de estados más probable hasta el instante de tiempo $t=1$, es decir, obtener el estado inicial más probable 
      \item Recursión: Calcular la probabilidad de la secuencia de estados más probable hasta el instante de tiempo $t$ a partir de la probabilidad de la secuencia de estados más probable
      hasta el instante de tiempo $t-1$.
      \item Terminación: Calcular la probabilidad de la secuencia de estados más probable hasta el instante de tiempo $T$.
      \item Reconstrucción de la secuencia de estados: Reconstruir la secuencia de estados más probable a partir de las probabilidades calculadas.
\end{itemize}

El uso de programación dinámica en este caso es claro, tener una matriz de probabilidades donde se almacenen la probabilidad máxima de cada estado hasta el instante de tiempo $t$,
lo que nos permitirá calcular la probabilidad máxima de cada estado hasta el instante de tiempo $t+1$ de forma eficiente. Por tanto tendremos una matriz $\mathscr{M}$ de $L$ filas 
(una para cada estado $q_k$ donde $k\in L$) y $T=L$ columnas (una para cada instante de tiempo $t\in T=L$). Recordemos que teníamos la secuencia de observaciones $O=\{o_1, o_2, \ldots, o_L\}$.\\ \\
La matriz $\mathscr{M}$ se inicializa de la siguiente forma:
\begin{itemize}
      \item $\mathscr{M}[k][1] = \pi_k \cdot b_k(o_1) \forall k \in N$, es decir, la probabilidad de que el estado $k$ sea el estado inicial multiplicado por la probabilidad de que se observe
      la primera observación en el estado $k$. Destacar que $N$ representa la cantidad de estados posibles ocultos en el modelo oculto de Markov.
\end{itemize}
Una vez inicializada la matriz $\mathscr{M}$, se procede a calcular las probabilidades de la secuencia de estados más probable hasta el instante de tiempo $t$ a partir de la probabilidad
de la secuencia de estados más probable hasta el instante de tiempo $t-1$. Para ello, se utiliza la siguiente fórmula:
\begin{itemize}
      \item $\mathscr{M}[j][t] = \max_{i=1}^{N} \left\{ \mathscr{M}[i][t-1] \cdot a_{ij} \cdot b_j(o_t) \right\}$, es decir, la probabilidad de la secuencia de estados más probable
      hasta el instante de tiempo $t-1$ multiplicada por
      la probabilidad de transición de $i$ a $j$ multiplicada por la probabilidad de observar $o_t$ en el estado $j$.
\end{itemize}



\chapter{Ejemplos de Uso}
El algoritmo de Viterbi es ampliamente utilizado en diversos campos tales como 
el lenguaje, la ingeniería de comunicaciones, la robótica, la biología, la medicina,
la meteorología, etc. A continuación presentamos algunas de las aplicaciones específicas
de este algoritmo en distintos campos.

\section{Lenguaje}
El algoritmo de Viterbi es utilizado en el reconocimiento de voz, en el reconocimiento
de escritura a mano, en la corrección de errores en el texto, en la traducción automática,
en la generación de texto, en la síntesis de voz, en la transcripción de audio, etc. Veamos
un ejemplo de uso de Viterbi en el proceso de desambiguación de palabras en un texto.
\subsection{Desambiguación de palabras}
Cuando queremos procesar el lenguaje natural, es común encontrarnos con palabras que tienen
múltiples significados. Por ejemplo, la palabra \textit{privado} está reconocida por la RAE como 
un adjetivo, un sustantivo y un verbo. Para desambiguar estas palabras cuando procesamos
un texto, podemos utilizar el algoritmo de Viterbi, en este caso particular, 
los elementos del modelo oculto de Markov serían:
\begin{itemize}
      \boldmath
      \item El conjunto $Q$ de estados ocultos (categorías gramaticales)
      \item El conjunto $V$ de estados observables (palabras)
      \item El conjunto $A$ de probabilidades de transición entre estados (probabilidades de cambio de categoría gramatical, por ejemplo, que un nombre vaya detras de un verbo)
      \item El conjunto $B$ de probabilidades de emisión de observaciones (probabilidades de que una palabra pertenezca a una categoría gramatical, por ejemplo, 
      que la palabra \textit{perro} sea un sustantivo es mucho mayor a que sea un adjetivo)
\end{itemize}
Mostramos un ejemplo para el texto \textit{quiero aprobar la asignatura}:
\begin{center}
\begin{tikzpicture}[every node/.style={draw, text height=1.5ex, text depth=.25ex}]
      % Nodes
      \node (s0) [rectangle, minimum width=1cm, minimum height=1cm] at (0,0) {estado 0};
      \node (verb1) [rectangle, minimum width=1cm, minimum height=1cm, right=1cm of s0] {verbo};
      \node (verb2) [rectangle, minimum width=1cm, minimum height=1cm, right=1cm of verb1] {verbo};
      \node (noun) [rectangle, minimum width=1cm, minimum height=1cm, right=1cm of verb2] {nombre};
      \node (noun2) [rectangle, minimum width=1cm, minimum height=1cm, right=1cm of noun] {nombre};
      \node (s1) [rectangle, minimum width=1cm, minimum height=1cm, right=1cm of noun2] {estado 0};
  
      % Solid lines
      \draw[->] (s0) -- (verb1);
      \draw[->] (verb1) -- (verb2);
      \draw[-, dashed] (verb2) -- (noun);
      \draw[-, dashed] (noun) -- (noun2);
      \draw[->] (noun2) -- (s1);
  
      % Text above nodes
      \node[draw=none, above=0.1cm of verb1] {quiero};
      \node[draw=none, above=0.1cm of verb2] {aprobar};
      \node[draw=none, above=0.1cm of noun] {la};
      \node[draw=none, above=0.1cm of noun2] {asignatura};
      
      % Possible transitions (dashed lines)
      \node (adv) [rectangle, minimum width=1cm, minimum height=1cm, below=1.5cm of verb1] {adverbio};
      \node (adj) [rectangle, minimum width=1cm, minimum height=1cm, below=1.5cm of noun] {adjetivo};
      \node (det) [rectangle, minimum width=1cm, minimum height=1cm, below=1.5cm of adj] {determinante};

      \draw[-, dashed] (s0) -- (adv);
      \draw[->] (verb2) -- (det);
      \draw[->] (det) -- (noun2);
      \draw[-, dashed] (adv) -- (verb2);
      \draw[-, dashed] (verb2) -- (adj);
      \draw[-, dashed] (adj) -- (noun2);
  \end{tikzpicture}
\end{center}
Donde, los observables son la secuencia de palabras \textit{quiero aprobar la asignatura} y los estados ocultos son las categorías gramaticales de las palabras,
que como se puede ver, contemplan sólo un conjunto limitado de categorías gramaticales. Esto se debe a que 
la probablidad de pertenencia de determinadas palabras a ciertas categorías gramaticales es 0, lo que simplifica
y acelera el proceso de desambiguación.

\subsubsection{Aplicación del Algoritmo de Viterbi}
El algoritmo de Viterbi se utiliza para encontrar la secuencia más probable de estados ocultos (categorías gramaticales) dada una secuencia de observaciones (palabras). Para nuestro ejemplo, el proceso se desarrolla de la siguiente manera:

\begin{itemize}
      \item Inicialización:
      \begin{itemize}
            \item Definir los estados ocultos posibles. Por ejemplo, en nuestro caso, los estados posibles son verbo, nombre, determinante, etc.
            \item Definir las probabilidades iniciales para cada estado oculto. Por ejemplo, es muy probable que la primera palabra quiero sea un verbo.
            \item $\pi(\text{verb}) = P(\text{verb} | \text{estado}_0)$.
      \end{itemize}
      \item Recursión:
      \begin{itemize}
            \item Para cada palabra en la secuencia, calcular la probabilidad de que cada posible estado oculto (categoría gramatical) siga a cada estado anterior. Esto se hace utilizando las probabilidades de transición ($A$) y las probabilidades de emisión ($B$).
            \item Ejemplo: Para la palabra aprobar, se calcularía $P(\text{verb}_2 | \text{verb}_1) \cdot P(\text{aprobar} | \text{verb}_2)$ para todas las categorías posibles de aprobar.
            \item Se selecciona el estado que maximiza esta probabilidad.
      \end{itemize}
      \item Terminación:
      \begin{itemize}
            \item Una vez procesadas todas las palabras, se selecciona la secuencia de estados que maximiza la probabilidad total de la secuencia observada.
            \item Esta secuencia representará las categorías gramaticales más probables para la oración.
      \end{itemize}
      \item Reconstrucción de la secuencia de estados:
      \begin{itemize}
            \item Utilizando las probabilidades calculadas, se reconstruye la secuencia de categorías gramaticales más probable.
            \item Por ejemplo: quiero (verbo), aprobar (verbo), la (determinante), asignatura (nombre).
      \end{itemize}
\end{itemize}


\end{document}
